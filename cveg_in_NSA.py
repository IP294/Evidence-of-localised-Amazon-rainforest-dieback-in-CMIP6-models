# -*- coding: utf-8 -*-
"""
Created on Sat Oct 16 19:10:54 2021

@author: impy2


pseudocode for the amount of cveg in the NSA region over time
- get the mat file for the cveg for each model
- mask out the NSA region
- for each point in time add up the total cveg in the NSA region
- plot this against time and see what happens

"""

from pathlib import Path
import iris
import netCDF4 as nc
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams.update({'font.size': 10})

from scipy.io import loadmat
import matplotlib.path as mpltPath

################# Specify the experiment and models ################

experiment = '1pctCO2'
variant_id = 'r1i1p1f1'

################# Specify variable of interest ######################
# var2 is the same as var but all lower case
var = 'cVeg' # or treeFrac
var2 = 'cveg' # or treefrac
var3 = 'tas'

############## Specify units of variable ###########################
units = '$kg/m^2$'

################# Specify model here ################################## 
models = ['EC-Earth3-Veg', 'GFDL-ESM4', 'MPI-ESM1-2-LR', 'NorCPM1', 'SAM0-UNICON', 'TaiESM1', 'UKESM1-0-LL']
colors = ['tab:blue', 'tab:green', 'tab:purple', 'darkred', 'dimgray', 'goldenrod', 'lightseagreen'] 
my_lats=np.arange(-90,91,step=1)
my_lons=np.arange(-180,181,step=1)

ny =  np.size(my_lats) 

comp_veg = np.zeros((np.size(models), 139))
comp_anom = np.zeros((np.size(models), 139))

##### Define latitude and longitude (uses 1 degree by 1 d) ######
nx = np.size(my_lons)

coord1 = iris.coords.DimCoord(my_lats,bounds=np.array([my_lats-0.5,my_lats+0.5]).T, standard_name='latitude', units='degrees', var_name='lat', attributes={'title': 'Latitude', 'type': 'double', 'valid_max': 90.0, 'valid_min': -90.0}, circular=True)

coord2 = iris.coords.DimCoord(my_lons,bounds=np.array([my_lons-0.5,my_lons+0.5]).T, standard_name='longitude', units='degrees', var_name='lon', attributes={'title': 'Longitude', 'type': 'double', 'valid_max': 180.0, 'valid_min': -180.0}, circular=True)

cube = iris.cube.Cube(np.zeros((ny,nx),np.float32),dim_coords_and_dims=[(coord1,0),(coord2,1)])

areas = iris.analysis.cartography.area_weights(cube, normalize=False)

##### initialise figure #######
fig, axes = plt.subplots(1, figsize=(2.7, 2.0))

for l in range(0, np.size(models)):
    model = models[l]

    if model == 'EC-Earth3-Veg':
    
        date_range = '1850-2000'
        
    elif model == 'GFDL-ESM4': 
        date_range = '1850-1999'
        
    elif model == 'MPI-ESM1-2-LR':
        
        date_range = '1850-2014'
    
    elif model == 'NorCPM1':
        
        date_range = '1850-2013'
    
    elif model == 'TaiESM1':
        
        date_range = '1850-2000'
    
    elif model == 'SAM0-UNICON':
        
        date_range = '1850-1999'
        
    elif model == 'UKESM1-0-LL':
        date_range = '1850-2000'
        variant_id = 'r1i1p1f2'   
        
    ##################### Set RELDIFF True if taking relative differennce #################
    ##################### False if absolute difference ####################################    
    RELDIFF = False
    
    # Is variable measured in per second?
    PERSEC = False
        
    ############ Specify name of directory of data which you want to plot #################
    region = 'Amazon'
    
    ############ Specify name of directory of that stores the figures #################
    region2 = 'Amazon'
    
    # Window length used to calculate abrupt shift over
    wl = 15
    
    ################## Specify path to processed data directory #####################
    path = 'C:/Users/impy2/OneDrive/Documents/Uni Yr3/Tipping Points Project/'+var+'/'+experiment+'/Processed_data_monthly/'+region+'/'
    path2 = 'C:/Users/impy2/OneDrive/Documents/Uni Yr3/Tipping Points Project/'+var+'/'+experiment+'/Analysis_data/'+region2+'/'
    
    #### load in monthly data ####
    fname = path+var+'_'+model+'_'+experiment+'_'+variant_id+'_'+date_range+'_mon_'+region+'.nc'
    fname = Path(fname)
    
    # # Load in data
    f = nc.Dataset(fname,'r')
    
    # Extract data and if measured in per second convert to per year
    if PERSEC:
        x = f.variables[var2][:]*3600*24*360
    else:
        x = f.variables[var2][:]
    
    # Close dataset
    f.close()           
    
    # load in mat file generated by abrupt_shift_detection_statistics 
    mat = loadmat(path2+var+'_'+model+'_as_grads_wl'+str(wl)+'_data_'+experiment+'_'+region2+'.mat')
        
    # Extract data from dictionary
    longitude, latitude, as_grad, as_grads, co2_tip, as_change, ovr_change = mat['Lon'], mat['Lat'], mat['as_grad'], mat['as_grads'], mat['co2_tip'], mat['as_change'], mat['ovr_change']
    
    #################### create masks of the Amazon regions ######################
    Lon, Lat = np.meshgrid(longitude[0,:], latitude[:,0])
    Lon_flat, Lat_flat = Lon.flatten(), Lat.flatten()
    points = np.vstack((Lon_flat,Lat_flat)).T
    
    # Vertices of the 4 Amazon boxes
    NWS_verts = [(-75,12), (-83.4,2.2), (-83.4,-10), (-79,-15), (-72,-15), (-72,12)]
    NSA_verts = [(-72,12), (-72,-8), (-50,-8), (-50,7.6), (-55,12)]
    NES_verts = [(-34,-20), (-50,-20), (-50,0), (-34,0)]
    SAM_verts = [(-66.4,-20), (-72,-15), (-72,-8), (-50,-8), (-50,-20)]
    
    nt = np.size(x, 0)
    ny = np.size(x, 1)
    nx = np.size(x, 2)
    
    # Create masks for each region (note nt, ny, nx, are dimension sizes of time, latitude, longitude respectively)
    NWS_path = mpltPath.Path(NWS_verts)
    NWS_grid = NWS_path.contains_points(points)
    NWS_grid = NWS_grid.reshape((ny, nx))
    NWS_mask = np.broadcast_to(NWS_grid, (nt, ny, nx))
    
    NSA_path = mpltPath.Path(NSA_verts)
    NSA_grid = NSA_path.contains_points(points)
    NSA_grid = NSA_grid.reshape((ny, nx))
    NSA_mask = np.broadcast_to(NSA_grid, (nt, ny, nx))
    
    NES_path = mpltPath.Path(NES_verts)
    NES_grid = NES_path.contains_points(points)
    NES_grid = NES_grid.reshape((ny, nx))
    NES_mask = np.broadcast_to(NES_grid, (nt, ny, nx))
    
    SAM_path = mpltPath.Path(SAM_verts)
    SAM_grid = SAM_path.contains_points(points)
    SAM_grid = SAM_grid.reshape((ny, nx))
    SAM_mask = np.broadcast_to(SAM_grid, (nt, ny, nx))
    
    # Create copies of surface temperature for each Amazon region
    NWS_cveg = x.copy()
    NSA_cveg = x.copy()
    NES_cveg = x.copy()
    SAM_cveg = x.copy()
    
    # Mask out all data not in Amazon box
    NWS_cveg[~NWS_mask] = np.nan
    NSA_cveg[~NSA_mask] = np.nan
    NES_cveg[~NES_mask] = np.nan
    SAM_cveg[~SAM_mask] = np.nan
    
        #make an array of areas of each of the grid points ###
    
    [rows, cols] = np.where(~np.isnan(NSA_cveg[0,:,:]))
    
    ## define a function to calculate the area of each grid point (km^2)
    r = 6357 # (km)
    
    def f_area(mid_lat):
        mid_lat_r = mid_lat*(np.pi/180)
        A = (((2* np.pi *r)/360)**2)*(np.cos(mid_lat_r))
        return A
    
    grid_areas = np.zeros(np.size(rows))
    for i in range (0, np.size(rows)):
        latitude = Lat[rows[i], cols[i]]
        grid_areas[i] = f_area(latitude) 
        
    ### convert areas to m^2 ####
    grid_areas = grid_areas*1e6
    
##### calculate cveg #####
 
    total_veg = np.zeros(np.size(NSA_cveg, 0)-1)
    veg_anom = np.zeros(np.size(NSA_cveg, 0)-1)
    
    for k in range (0, np.size(NSA_cveg, 0)-1):
        veg = NSA_cveg[k,:,:].flatten()
        veg = veg[~np.isnan(veg)]
        veg_areas = veg*grid_areas
        total_veg[k] = np.sum(veg_areas)
        veg_anom[k] = total_veg[k] - total_veg[0]
    
    ## calculate the yearly average ##
    veg_avr = np.zeros(int(np.size(veg_anom, 0)/12))
    for i in range (np.size(veg_avr)):
        veg_avr[i] = np.average(veg_anom[i*12:(i*12)+12])
        # veg_avr[i] = np.average(total_veg[i*12:(i*12)+12])
    
    ###### convert to peta grams from kg #####
    veg_avr = veg_avr/1e12
    
    ############## calcualte the temperature anomolies over time ################
    path4 = 'C:/Users/impy2/OneDrive/Documents/Uni Yr3/Tipping Points Project/'+var3+'/'+experiment+'/Processed_data_monthly/'
    region1 = 'World'
    
    # File name of interpolated World data
    fname = path4+region1+'/'+var3+'_'+model+'_'+experiment+'_'+variant_id+'_'+date_range+'_mon_'+region1+'.nc'
    # Load in world data into iris cube
    world_cube = iris.load_cube(fname,var3)
    
    world_data = world_cube.data
    
    ##### area weight the temperature #####
    weighted_tas =np.zeros((np.size(world_data,0), np.size(world_data,1), np.size(world_data,2)))
    weighting = areas/np.max(areas)
    for i in range (0, np.size(world_data,0)):
        weighted_tas[i] = world_data[i,:,:]*weighting
    
    #################### average temperature across the globe ####################
    
    pavr = np.zeros(np.size(weighted_tas, 0))
    for i in range (np.size(weighted_tas, 0)):
        t_point = weighted_tas[i,:,:]
        t_point = t_point.flatten()
        t_point = t_point[~np.isnan(t_point)]
        pavr[i] = np.average(t_point)
        
    tavr = np.zeros(int(np.size(weighted_tas, 0)/12))
    for i in range (np.size(tavr)):
        tavr[i] = np.average(pavr[i*12:(i*12)+12])                
        
    ### first 10 years of the model is the ref temp
    ref = np.average(tavr[0:10])
    
    anom = np.zeros(np.size(tavr)-10)
    ### calculate the temperaure anomaly for each 10 year window going forward
    for i in range (0, np.size(tavr)-10):
        temp = np.average(tavr[i:i+10])
        anom[i] = temp - ref
     
    comp_veg[l, :] = veg_avr[10:149]
    comp_anom[l,:] = anom[0:139]
    
   
    axes.plot(anom[0:-1], veg_avr[10::], label = models[l], color = colors[l], linewidth=0.8)

#create arrays for compliled models line
compiled_veg = np.zeros(139)
compiled_anom = np.zeros(139)
   
#### create x axis coordinates to interpolate onto 
x_interp = np.arange(0, np.max(comp_anom), step = np.max(comp_anom)/np.size(comp_anom,1))

#### interpolate the data ####
interp_anom = np.zeros((np.size(comp_veg,0), np.size(comp_veg, 1)))

for i in range (0, np.size(interp_anom,0)):
    interp_anom[i,:] = np.interp(x_interp, comp_anom[i,:], comp_veg[i,:])
    
for i in range (0, np.size(compiled_veg)):
    compiled_veg[i] = np.sum(interp_anom[:, i])/np.size(models)
    # compiled_anom[i] = np.sum(comp_anom[:,i])/np.size(models)
    
# plot compiled models line    
axes.plot(x_interp, compiled_veg, label='Compiled Models', color='black', linewidth=1.8)

##### calculate error in each index of the array ######
err = np.zeros(np.size(interp_anom,1))
for i in range (0,np.size(interp_anom,1)):
    err[i] = np.std(interp_anom[:,i])

# plot the error in compiled models
up_err = (compiled_veg)+err
low_err = (compiled_veg)-err
axes.fill_between(x_interp, low_err, up_err, alpha=0.25, color='black')
    
plt.xlabel('Global Warming ('+ u'\u2103'+')')
plt.ylabel('Vegetetation carbon \n Anomaly ($PgC$)')
plt.yticks(np.arange(-40, 60, step=20))
plt.xticks(np.arange(0,4.5, step=0.5))
plt.xlim([0,3.0])
fig.text(0.85, 0.79, '(c)', ha='center')
plt.show()
        
# savepath = 'C:/Users/impy2/OneDrive/Documents/Uni Yr3/Tipping Points Project/Figures/Paper/Figure 2/'
# filename = savepath+'cveg_in_NSA_V7.svg'
# fig.savefig(filename, format = 'svg', dpi=300, bbox_inches='tight') 

    
    
    


